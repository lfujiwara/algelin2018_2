\documentclass[12pt]{article}
\usepackage[right=2cm, left=2cm, top=2cm]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{framed}
\usepackage[utf8]{inputenc}
\author{BCC IME-USP 2018}
\title{Álgebra Linear - Resumo P2}

\begin{document}
\maketitle
\section{Aula 9 - Sistemas Lineares}
\paragraph{Sistemas Lineares - Definição}

	\begin{itemize}
	\item Um sistema linear é uma equação de forma $AX = B$, tal que:\\
	\fbox{\parbox{350pt}{
	$A$ é uma matriz com $n$ linhas e $m$ colunas, $B$ e $X$ são da forma:\\
	$
	B = b^t = \begin{bmatrix} b_1 \\ \vdots \\ b_n \end{bmatrix};
	X = \begin{bmatrix} x_1 \\ \vdots \\ x_m \end{bmatrix} 
	$
	}}
	
	\item Tal sistema se equivale à:
	$ \left\lbrace
	\begin{matrix} 
	a_{11}x_1 & + & \dots & + & a_{1m}x_m = b_1\\
	\vdots & & \ddots & & \vdots \\
	 a_{n1}x_1 & + & \dots & + & a_{nm}x_m = b_n\\
	\end{matrix}\right.
	$
	
	\item Chamamos $A$ de matriz dos coeficientes do sistemas e a matriz Â com \\[5pt]
	Â $= \begin{bmatrix}
	a_{11} & \dots & a_{1m} & b_1 \\
	\vdots & \ddots & \vdots \\
	a_{n1} & \dots & a{nm} & b_n
	\end{bmatrix}$
	de \textit{matriz estendida} do sistema linear.

	\item Chamamos o sistema de sistema homogêneo se, e somente se $B = \overrightarrow{0}$.
	\end{itemize}
	
\paragraph{Sistemas Lineares - Propriedades e outros}

\begin{itemize}
	\item Equivalência sobre troca de linhas $L_i \leftrightarrow L_j$:\\
		Dado um sistema linear $AX=B$, um sistema linear denotado por\\
		$A_{L_i \leftrightarrow L_j}X = B_{L_i \leftrightarrow L_j}$, onde $A_{L_i \leftrightarrow L_j}$ é
		a matriz $A$ com as linhas $i$ e $j$ trocadas, e o mesmo realizado com $B_{L_i \leftrightarrow L_j}$.
		Isto é, as soluções de ambos os sistemas são idênticas.
		
	\item Equivalência sobre troca de linhas $L_i \leftrightarrow L_i - \lambda L_j$:\\
		A propriedade acima segue (equivalência) se a linha $i$ for trocada por uma combinação linear $L_i - \lambda L_j$.
		
	\item Definimos as operações de trocas de linhas $L_i \leftrightarrow L_j$ em $A,B$ como operações elementares sobre qualquer
	sistema $AX=B$.
	
	\item Matriz escalonada - Definição:
	\begin{itemize}
		\item[a.] Se a linha $i$ for nula, todas as linhas abaixo de $i$ ($j \vert j > i)$) são nulas.
		\item[b.] A primeira entrada não nula de cada linhas é 1.
		\item[c.] Utilizando as operações elementares acima, podemos escalonar qualquer matriz de um sistema linear, em especial, procuraremos
		escalonar matrizes estendidas, pois, dessa forma, mantemos a equivalência do sistema linear, pois realizaremos as mesmas mudanças
		para $B$.
	\end{itemize}
	
	\item Soluções de sistemas lineares não homogêneos:
	\begin{itemize}
		\item Considere o sistema linear não homogêneo $AX_0 = B$.
		\item Considere $X_0$, uma solução particular desse sistema, e $W$ o espaço de soluções do sistema homogêneo associado
		($AX = \overrightarrow{0}$).
		\item Então, existe um $w \in W$ tal que, sendo $X_0$ e $X$ soluções particulares do sistema não homogêneo, vale $X = X_0 + w$.
		\item O que queremos dizer aqui é que dada uma solução particular do sistema não homogêneo, as outras soluções do sistema podem
		ser construída com essa solução encontrada mais alguma solução do sistema homogêneo associado.
	\end{itemize}
\end{itemize}

\paragraph{Sistemas Lineares - Exemplos}

\subparagraph{Exemplo 1:}
	\begin{itemize}
	\item Considere o sistema:
	$\begin{bmatrix}
	1 & -1 & 1 & 2 \\ 2 & 1 & 3 & 2 \\ 1 & 5 & 3 & -2
	\end{bmatrix}$
	$\begin{bmatrix}
	x \\ y \\ z \\ w
	\end{bmatrix}$
	$ = $
	$\begin{bmatrix}
	0 \\ 0 \\ 0 
	\end{bmatrix}$
	\item Sua matriz estendida:
	$\begin{bmatrix}
	1 & -1 & 1 & 2 & 0\\ 2 & 1 & 3 & 2 & 0\\ 1 & 5 & 3 & -2 & 0
	\end{bmatrix}$
	
	\item Escalonando a matriz estendida:
		\begin{itemize}
		\item $\begin{bmatrix}
		1 & -1 & 1 & 2 & 0 \\ 2 & 1 & 3 & 2 & 0 \\ 1 & 5 & 3 & -2 & 0
		\end{bmatrix}$
		$\begin{matrix} L_2 \leftrightarrow L_2 - 2L_1 \\ L_3 \leftrightarrow L_3 - L_1 \end{matrix}$
		$\begin{bmatrix}
		1 & -1 & 1 & 2 & 0 \\ 0 & 3 & 1 & -2 & 0 \\ 0 & 6 & 2 & -4 & 0
		\end{bmatrix}$
		
		\item $\begin{bmatrix}
		1 & -1 & 1 & 2 & 0 \\ 0 & 3 & 1 & -2 & 0 \\ 0 & 6 & 2 & -4 & 0
		\end{bmatrix}$
		$\begin{matrix} L_3 \leftrightarrow L_3 - 2L_2 \end{matrix}$
		$\begin{bmatrix}
		1 & -1 & 1 & 2 & 0 \\ 0 & 3 & 1 & -2 & 0 \\ 0 & 0 & 0 & 0 & 0
		\end{bmatrix}$
		
		\item $\begin{bmatrix}
		1 & -1 & 1 & 2 & 0 \\ 0 & 3 & 1 & -2 & 0 \\ 0 & 0 & 0 & 0 & 0
		\end{bmatrix}$
		$\begin{matrix} L_2 \leftrightarrow L_2 - \frac{2}{3}L_2 \end{matrix}$
		$\begin{bmatrix}
		1 & -1 & 1 & 2 & 0 \\ 0 & 1 & \frac{1}{3} & -\frac{2}{3} & 0 \\ 0 & 0 & 0 & 0 & 0
		\end{bmatrix}$
		\end{itemize}
	
	\item Temos o sistema equivalente:\\
	$\begin{bmatrix}
	1 & -1 & 1 & 2 \\ 0 & 1 & \frac{1}{3} & -\frac{2}{3} \\ 0 & 0 & 0 & 0
	\end{bmatrix}$	
	$\begin{bmatrix}
	x \\ y \\ z \\ w
	\end{bmatrix}$	
	$=$
	$\begin{bmatrix}
	0 \\ 0 \\ 0
	\end{bmatrix}$	
	
	\item Ou seja:
	$	\begin{cases}
	x - y + z + 2w = 0 \\
	y + \frac{1}{3}z - \frac{2}{3}w = 0
	\end{cases}$	
	
	\item
	$ \begin{cases}
	y = - \frac{1}{3}z + \frac{2}{3}w \\
	x = y - z - 2w = -\frac{4}{3}z - \frac{4}{3}w \\
	\end{cases}$
	
	\item Logo, temos a solução:
	$(x,y,z,w) = \left( -\frac{4}{3}z - \frac{4}{3}w, - \frac{1}{3}z + \frac{2}{3}w, z, w \right) \\ =
	z \left( -\frac{4}{3}, -\frac{1}{3}, 1, 0 \right) + w \left( -\frac{4}{3}, \frac{2}{3}, 0, 1 \right)
	$
	
	\item E o espaço de soluções: $Span_{\mathbb{K}} \left[ \left( -\frac{4}{3}, -\frac{1}{3}, 1, 0 \right),
	\left( -\frac{4}{3}, \frac{2}{3}, 0, 1 \right) \right] $
	
	\end{itemize}
\newpage
\section*{Aula 10 - Determinantes}
\paragraph{Introdução\\}
	Dada uma matriz quadrada	de tamanho $n$ com entradas em um corpo $\mathbb{K}$, ou seja, um elemento
	do $\mathbb{K}$-espaço vetorial $V = \mathbb{K}^n \times \dots \times \mathbb{K}^n$, o determinante é uma função
	$det: V \rightarrow \mathbb{K}$. Melhor dizendo, procuramos uma função em que \textbf{podemos enviar uma matriz e receber
	um outro valor pertencente ao corpo que a matriz possui entradas}, como, por exemplo, temos uma matriz com entradas em $				\mathbb{R}$ e então podemos ligá-la ao $\mathbb{R}$.
	($V \rightarrow \mathbb{R}$)
	
\paragraph{Definindo a função (casos $n^\prime = 1$ até $n^\prime = 3$)}
\begin{enumerate}
	\item Considere a matriz $A = (a_{11})$, representante do sistema $ax=b$, assumimos que $det(A) = a_{11}$, vale notar que a condição fundamental para que o sistema seja possível (dado $b\neq 0$) é que $a \neq 0$.
	
	\item Considere a matriz $A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$, claramente, pode representar o 
sistema $\begin{cases} ax + by = r \\ cx + dy = s \end{cases}$, calculando o valor de $x$, temos
$(ad - bc)x = rd - sb$ e para $y$, $(ad - bc)y = bs - rc$. Para o sistema ser resolvido, temos a condição fundamental
$ad-bc \neq 0$. Nota-se, portanto, que $ad - bc$ deve ser um \textbf{invariante} da matriz, o qual define uma condição fundamental para que a mesma seja resolvida, temos, então, $det(A) = ad - bc$.

	\item Considere as matrizes\\
	$A = \begin{bmatrix}
	a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33} \end{bmatrix}$ e $B = \begin{bmatrix} b_1 \\ b_2 \\ b_3 \end{bmatrix}$\\ Definimos com essas matrizes o sistema $\begin{cases} a_{11}x_1 + a_{12}x_2 + a_{13}x_3 = b_1 \\ a_{21}x_1 + a_{22}x_2 + a_{23}x_3 = b_2 \\ a_{31}x_1 + a_{32}x_2 + a_{33}x_3 = b_3 \end{cases}$\\[10pt]
	Para resolver o sistema, dividimos em: \\[10pt] $\begin{cases} a_{11}x_1 + a_{12}x_2 + a_{13}x_3 = b_1 \\ a_{21}x_1 + a_{22}x_2 + a_{23}x_3 = b_2\end{cases},\begin{cases} a_{21}x_1 + a_{22}x_2 + a_{23}x_3 = b_2 \\ a_{31}x_1 + a_{32}x_2 + a_{33}x_3 = b_3 \end{cases}$\\[10pt] e resolvemos cada um como fizemos no caso anterior, resultando em:\\[10pt] $\begin{cases} (a_{21}a_{12} - a_{22}a_{11})x_2 + (a_{21}a_{13} - a_{11}a_{23})x_3 = b_3 \\ (a_{31}a_{22} - a_{21}a_{32})x_2 + (a_{31}a_{23} - a_{21}a_{33})x_3 = b_3 \end{cases}$ \\[10pt]
	Como condição para possibilidade do sistema, resolvendo de forma análoga ao item anterior, temos:\\
	$(a_{21}a_{12} - a_{22}a_{11})(a_{31}a_{23} - a_{21}a_{33}) - (a_{31}a_{22} - a_{21}a_{32})(a_{21}a_{13} - a_{11}a_{23})$
	\\[5pt]
	$= a_{21}a_{12}a_{23}a_{31} - a_{21}a_{12}a_{21}a_{33} - \underline{a_{22}a_{11}a_{23}a_{31}} + a_{21}a_{11}a_{22}a_{33} \\	
	- a_{21}a_{13}a_{22}a_{31} + \underline{a_{22}a_{11}a_{23}a_{31}} + a_{21}a_{13}a_{21}a_{32} - a_{21}a_{11}a_{23}a_{32}$ \\[10pt]
	Eliminando os zeros e o elemento em comum (o qual não deve influenciar o resultado da função, pois procuramos um invariante), temos:\\
	$= a_{12}a_{23}a_{31} - a_{12}a_{21}a_{33} + a_{11}a_{22}a_{33} - a_{13}a_{22}a_{31} + a_{13}a_{21}a_{32} - a_{11}a_{23}a_{32}$\\[10pt]
	$= a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - (a_{12}a_{21}a_{33} + a_{13}a_{22}a_{31} + a_{11}a_{23}a_{32})$
\end{enumerate}\

\paragraph{Estudando os casos dados}
\begin{itemize}
	\item Para estudar os casos dados, revisitamos os grupos de permutações, para o caso $n = 1$, é trivial, para o caso $n=2$, temos a matriz:
	$\begin{bmatrix} 1 & 2 \\ \sigma(1) & \sigma(2) \end{bmatrix}$ representando uma permutação.\\[10pt]
	Obviamente, sabemos que a quantidade de permutações para $n=2$ é 2, sendo a permutação netura, em que nada é alterado, portanto, $\begin{bmatrix} 1 & 2 \\ 1 & 2 \end{bmatrix} = \sigma_1 \leftrightarrow a_{11}a_{22}$, a permutação em que trocamos de lugar os elementos (apenas uma nesse caso), $\begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix} = \sigma_2 \leftrightarrow a_{12}a_{21}$ (aqui associamos uma permutação com um dos termos do determinante da matriz 2$\times$2. Note que a permutação neutra ($\sigma_0$) se associa com o sinal positivo, e a permutação 1 ($\sigma_1$) se associa com o sinal negativo.

	\item Definimos aqui o conceito da paridade de permutações, dada uma permutação, sua paridade é definida pela quantidade de inversões que a mesma possui, isto é, todos os pares $x,y$ tais que $x < y$ e $\sigma(x) > \sigma(y)$. Por exemplo, na permutação $\sigma = \begin{bmatrix} 1 & 2 & 3 \\ 3 & 1 & 2 \end{bmatrix}$, analisamos os pares $(1,2),(1,3),(2,3)$:
	\begin{itemize}
		\item $(1,2)$: $1 < 2$; $\sigma(1) = 3$, $\sigma(2) = 1 \rightarrow \sigma(1) > \sigma(2) \rightarrow$  1 inversão.
		\item $(1,3)$: $1 < 3$; $\sigma(1) = 3$, $\sigma(3) = 2 \rightarrow \sigma(1) > \sigma(3) \rightarrow$  1 inversão.
		\item $(2,3)$: $2 < 3$; $\sigma(2) = 1$, $\sigma(3) = 2 \rightarrow \neg (\sigma(2)>\sigma(3)) \rightarrow$ sem inversão.
		\item Total: 2 inversões
	\end{itemize}
		
	\item O sinal associado à cada permutação, e, por consequência, a um termo do determinante, é relacionado com a paridade de sua permutação, se esta é par, o sinal é positivo, se não, é negativo, ou seja: $sign(\sigma)=(-1)^{n_i}$, sendo $n_i$ a quantidade de inversões da permutação. No caso anterior, temos $n_i = 2$, portanto $sign(\sigma) = (-1)^2$, ou seja, positivo.	
\end{itemize}

\paragraph{Definição do determinante\\}
	Podemos, então, definir a função determinante:\\[10pt]
	\centerline{\fbox{
	$det(A) = \sum\limits_{\sigma \in S_n} sign(\sigma) a_{{1\sigma(1)}} a_{{2\sigma(2)}} \dots a_{{n\sigma(n)}}$	
	}}
	
\newpage
\section*{Aula 11 - Determinantes (parte 2)}
\paragraph{Permutações\\}
	Considere os polinômios das variáveis $x_1, x_2, x_3$:
	\begin{center}
	$P_2(x_1,x_2) = x_1 - x_2$ \\[5pt] 
	$P_3(x_1,x_2,x_3) = (x_1 - x_2)(x_2 - x_3)$ \\[5pt]
	$P_n(x_1, \dots, x_n) = \prod\limits_{1 \leq i < j \leq n} (x_i - x_j)$
	\end{center}
	e defina:
	\begin{center}
	$\sigma P_2(x_1, x_2) = P_2(x_{\sigma(1)}, x_{\sigma(2)})$\\
	$\sigma P_3(x_1, x_2, x_3) = P_3(x_{\sigma(1)}, x_{\sigma(2)}, x_{\sigma(3)})$
	\end{center}
	Para o caso geral:\begin{center}$P_n(x_{\sigma(1)}, \dots, x_{\sigma(n)}) = \prod\limits_{1 \leq i < j \leq n} (x_{\sigma(i)} - x_{\sigma(j)})$\end{center}
	Note que $\sigma P_n \in \lbrace P_n, -P_n \rbrace$, pois a diferença dois-a-dois entre todos os monômios está definida, então, qualquer permutação apenas troca o sinal dessa diferença, é trivial que, portanto, para um número par de trocas, o sinal será positivo, e, caso contrário, será negativo.\\
	Logo, temos:
	\begin{center}
	$\sigma P_n = sign(\sigma) P_n$ \\[5pt]
	$sign(\sigma \tau) = \frac{\sigma \tau P_n}{P_n} = \frac{\sigma (sign(\tau)P_n)}{P_n} = sign(\tau)\frac{sign(\sigma) P_n}{P_n}= sign(\sigma) sign( \tau )$
	\end{center}
	Do último, temos que $sign: S_n \rightarrow \lbrace 1, -1 \rbrace$ é um homomorfismo de grupos, pois $sign(\sigma * \tau) = sign(\sigma) \times sign(\tau)$ (preserva as operações dos grupos).\\
	Vale dizer que no. de permutações pares = ímpares = $\frac{n!}{2}$

\paragraph{Ciclos\\}
		Um $k$-ciclo é um elemento $\alpha \in S_n$ que "movimenta" $k \geq 2$ elementos, $i_1, \dots, i_k$ de $\lbrace 1, \dots, n \rbrace$ da seguinte forma:
		\begin{center}
		$\begin{cases}
			\alpha(i_j) = i_{j+1}$, se $1 \leq j \leq k - 1 \\
			\alpha(i_k) = i_1 \\
			\alpha(l) = l, \forall l \notin \lbrace i_1, \dots, i_k \rbrace
		\end{cases}$
		\end{center}
		Nesse caso, a notação para esse $k$-ciclo é $\alpha(i_1 \dots i_k)$ e o conjunto chamado de \textbf{suporte do ciclo} é $supp(\alpha)=\lbrace i_1, \dots, i_k \rbrace$.\\
		Exemplificando, o 2-ciclo é uma transposição, onde apenas troca 2 elementos de lugar, ao passo que um 3-ciclo (134) para $n = 4$ é a permutação:
		\begin{center}
			$\begin{bmatrix}
				1 & 2 & 3 & 4 \\ 3 & 2 & 4 & 1
			\end{bmatrix}$
		\end{center}
		Note que dois ciclos disjuntos, isto é, seus respectivos suportes são disjuntos ($supp(\alpha) \cap supp(\beta) = \emptyset$), comutam, ou seja, $\alpha \beta = \beta \alpha$, ciclos com tal propriedade são chamados de \textbf{ciclos disjuntos}.\newpage
		Concluindo, temos:
		\begin{enumerate}
			\item Existem ciclos disjuntos $\alpha_1, \dots, \alpha_r$ tais que $\alpha = \alpha_1 * \dots * \alpha_r$
			\item Cada ciclo $\alpha_j$ é um produto de $m_j$ transposições.
			\item $\alpha$ é um produto de $m_1 + \dots + m_r$ transposições.
			\item $sign(\alpha) = (-1)^m$
			\item $\alpha$ é permutação par se, e somente se $m$ é par.
		\end{enumerate}

\paragraph{Determinantes - Redução para dimensões menores\\}
	Na aula anterior, foi visto para o exemplo $n = 3$ que o cálculo do determinante foi reduzido para um exemplo de $n = 2$, o objetivo dessa seção é mostrar como reduzir de uma dimensão $n \in \mathbb{N}$ para $n - 1$.
	\begin{itemize}
		\item Definição: \textbf{Minor}\\
		Dada uma matriz $A \in M_n(\mathbb{K})$ (Matriz quadrada $n$ por $n$, com entradas no corpo $\mathbb{K}$), seu \textbf{minor} $A_{ij}$ ($1 \leq i,j \leq n$) é a matriz que obtém retirando a linha $i$ e coluna $j$ de $A$, claramente, reduzindo seu tamanho de $n$ para $n-1$. Exemplo:
		\begin{center}
		$A = \begin{bmatrix}		
		a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33}
		\end{bmatrix}$\\[10pt]
		$A_{11} = \begin{bmatrix} a_{22} & a_{23} \\ a_{32} & a_{33} \end{bmatrix}$ 
		$A_{21} = \begin{bmatrix} a_{12} & a_{13} \\ a_{32} & a_{33} \end{bmatrix}$ 
		$A_{31} = \begin{bmatrix} a_{12} & a_{23} \\ a_{32} & a_{33} \end{bmatrix}$ 
		$A_{12} = \begin{bmatrix} a_{21} & a_{23} \\ a_{31} & a_{33} \end{bmatrix}$
		\end{center}
		
		\item Definição: \textbf{Cofator}\\
		Definimos o cofator($i,j$) como sendo
		\begin{center}
			$d_{i,j} = (-1)^{i+j} det(A_{ij})	$
		\end{center}
		
		\item Fórmula: \textbf{Determinante de uma matriz $A$ à partir da redução de sua dimensão}\\
		Pelo resultado anterior, sendo $j_0$ uma coluna fixada temos:
		\begin{center}
			$det_{n+1}(A) = \sum\limits_{\sigma\in S_n} (-1)^{sign(\sigma)} a_{1\sigma(1)} a_{2\sigma(2)} \dots a_{n\sigma(n)}$\\
			$= \sum\limits_{i = 1}^n (-1)^{i+j_0} a_{ij_0} det(A_{ij_0})$\\
			$= \sum\limits_{i = n}^n a_{ij_0}d_{ij_0}$
		\end{center}
	\end{itemize}
	
\newpage
\paragraph{Determinantes - $det(A) = 0$ quando uma linha (ou coluna) é repetida\\}
	Tal propriedade provém do caso base $n = 2$, considere $A = \begin{bmatrix} a & b \\ a & b \end{bmatrix}$, calculando seu determinante, temos $ab - ab = det(A) = 0$, sabemos também que o determinante de qualquer matriz, por exemplo, para uma matriz $n=3$, pode ser expresso como somas/diferenças de determinantes de matrizes 2x2, igualando a zero para $n=3$, e assim por diante (uma prova formal é um bom exercício).
	
\paragraph{Determinantes - Outras Propriedades}
	\begin{itemize}
		\item $det(A) =  det(A^t)$
		\item $cof(A)$ é a \textbf{matriz dos cofatores} de $A$, isto é, a matriz obtida substituindo cada entrada de $A$ por seu cofator.
		\item $adj(A) = cof(A)^t$ é a \textbf{matriz adjunta} de $A$, isto é, a transposta da matriz dos cofatores de $A$.
		\item $\frac{1}{det(A)} adj(A) A = Id$ ($Id$ é a matriz identidade), logo, temos:
		\begin{center}
			$A^{-1} = \frac{adj(A)}{det(A)}$
		\end{center}
	\end{itemize}
	
\newpage
\section*{Aula 12 - Determinantes (parte 3)}
\paragraph{Determinantes - Interpretação geométrica\\}
	Seja $A \in M_3(\mathbb{R})$, no curso de Vetores e Geometria foi visto que esta pode representar 3 vetores $\in \mathbb{R}^3$, e seu determinante representa o produto misto entre esses vetores. Relembramos aqui que tal produto misto representa o volume do paralelepído formado por estes 3 vetores.\\
	Definimos aqui o volume de um sólido no $\mathbb{K}^n$-espaço vetorial, o mesmo deve ser definido pelos vetores $v_1, \dots, v_n$:
	\begin{center}
		$P(v_1, \dots, v_n) = \left\lbrace \sum\limits^n_{i=1} x_i v_i: x_i (0 \leq x_i \leq 1), \forall 1 \leq i \leq n \right\rbrace$
	\end{center}
	O volume será definido pelo produto misto entre esses vetores, o qual pode ser expresso por:
	\begin{center}
		$volume(P(v_1,\dots,v_n)) = \vert [v_1,\dots,v_n] \vert = \vert det_n(A) \vert$,\\
		onde $A$ é matriz cujas colunas são os vetores $v_1,\dots,v_n$
	\end{center}

\paragraph{Determinantes - Propriedades gerais}
	\begin{enumerate}
		\item $det(A)$ é invariante sob operações elementares (definidas na Aula 9) de linhas e colunas de A. Se houverem trocas de linhas, cada troca de linhas inverte o sinal do determinante, e, caso haja multiplicação de uma linha por escalar $\lambda$, o determinante de $A'$ será $det(A') = \lambda det(A)$.
		\item $det_n(A) = \sum\limits^n_{i=1} a_{ij_0} d_{ij_0} = \sum\limits^n_{j=1} a_{i_0j} d_{i_0j}$, onde $d_{ij}$ são cofatores ($i,j$) de $A$.
		\item $det(A) = det(A^t)$ e $det(Id) = 1$
		\item O sistema linear $AX = B$ tem uma solução única se, e somente se $det(A) \neq 0$
		\item $det(AB) =	det(A) \cdot det(B)$
		\item $adj(A) \cdot A = A \cdot adj(A) = det(A) \cdot Id$
		\item $A$ é invertível se, e somente se $det(A) \neq 0$, e, nesse caso: $A^{-1} = \frac{1}{det(A)} \cdot adj(A)$
	\end{enumerate}

\paragraph{Grupo Linear Geral e Grupo Linear Especial\\}
	Definimos o $GL$ (General Linear) e $SL$ (Special Linear):
	\begin{center}
		$GL_n(\mathbb{K}) = \lbrace A \in M_n(\mathbb{K}): det(A) \neq 0 \rbrace$\\[5pt]
		$SL_n(\mathbb{K}) = \lbrace A \in M_n(\mathbb{K}): det(A) = 1 \rbrace$
	\end{center}
	
\newpage
\section*{Aula 13}
\paragraph{Homomorfismo de grupos - Definição\\}
	Dados dois grupos $(G_1,*)$ e $(G_2,\circ)$. Um homomorfismo de grupos é uma função:
	\begin{center} $f: G_1 \rightarrow G_2$ \end{center}
	que satisfaz as seguintes propriedades, sendo $e_1$ e $e_2$ os elementos neutros dos grupos:
	\begin{center} $f(x * y) = f(x) \circ f(y)$ \\ $f(e_1) = e_2$ \end{center}
	Isto é, a $f$ preserva as operações desses grupos, temos, de imediato, que:
	\begin{center} $f(x^{-1}) = [f(x)]^{-1}$ \end{center}

\paragraph{Núcleo e Imagem - Definição\\}
	Definimos aqui o núcleo e imagem de um homomorfismo de grupos $f: G_1 \rightarrow G_2$, considere $e_2$ como elemento neutro de $G_2$:
	\begin{center} 
		$Ker(f) = \lbrace x \in G_1: f(x) = e_2 \rbrace \subset G_1$\\[5pt]
		$Im(f) = \lbrace f(x): x \in G_1 \rbrace \subset G_2$
	\end{center}
	São subgrupos e respectivamente chamados de núcleo ($Kernel$) e imagem ($Image$) de $f$.

\paragraph{Homomorfismo de grupos - Exemplos}
	\begin{enumerate}
		\item Considere $sign: (S_n, \circ) \rightarrow (\lbrace 1,-1 \rbrace, \cdot)$.\\
		Sabemos que $sign$ satisfaz, sendo $e$ a permutação $\in S_n$ neutra: \begin{center} $sign(a \circ b) = sign(a) \cdot sign(b)$ \\[5pt] $sign(e) = 1$ \end{center}
		\item Considere $det_n: GL_n(\mathbb{K}) \rightarrow (\mathbb{K}^*,\cdot)$, é um homomorfismo pois:
			\begin{center}
				$det(Id) = \overrightarrow{1}$ \\
				$det(AB) = det(A) \cdot det(B)$
			\end{center}
		Note que: $Ker(det_n) = SL_n(\mathbb{K})$
	\end{enumerate}
	
\paragraph{Transformações Lineares e Funcionais Lineares - Definição\\}
		Sabemos que um espaço vetorial é um grupo abeliano sobre o qual é definida uma segunda operação, denotada por \textbf{multiplicação por escalar}. Aqui estendemos a noção de homomorfismos de grupos para espaços vetoriais, nessa situação, claramente é preciso levar em conta a multiplicação por escalar. Considere $+$ como a operação num grupo abeliano.\\
		Sejam $V_1$ e $V_2$ espaços vetoriais sobre um corpo $\mathbb{K}$. Uma \textbf{transformação linear} entre $V_1$ e $V_2$ é sempre denotada por:
		\begin{center}
			$T: V_1 \rightarrow V_2$
		\end{center}
	Tal que \begin{enumerate}
	\item $ T(v + w) = T(v) + T(w)$
	\item $T(\lambda v) = \lambda T(v)$
	\end{enumerate}
	Se $V_2 = \mathbb{K}$, então $T$ é um funcional linear e denotamos como $T: V_1 \rightarrow \mathbb{K}$.\\
	Utilizamos o nome \textbf{linearidade} de $T$ para nos referir às propriedades citadas acima:
	\begin{center}
		$T \left( \sum\limits_{i=1}^m \lambda_i v_i \right) =  \sum\limits_{i=1}^m \lambda_i T(v_i) $
	\end{center}

\paragraph{Transformações Lineares e Funcionais Lineares - Núcleo e Imagem\\}
	Analogamente ao que foi definido para grupos, temos para Transformações e Funcionais Lineares:
	\begin{center}
		$Ker(T) = \lbrace x \in V_1: T(x) = \overrightarrow{0} \rbrace \subset V_1$ \\[5pt]
		$Im(T) = \lbrace T(x) \in V_2: x \in V_1 \rbrace \subset V_2$
	\end{center}
	Denotando $\overrightarrow{0}$ como o vetor nulo de ambos os espaços vetoriais, temos:
	\begin{center}
		$T(\overrightarrow{0}) = \overrightarrow{0}$
	\end{center}
	Note que $Ker(T)$ e $Im(T)$ são subgrupos e subespaços vetoriais.

\paragraph{Nulidade e Posto - Definição\\}
	Seja $T: V_1 \rightarrow V_2$ uma transformação linear entre $\mathbb{K}$-espaços vetoriais, definimos:
	\begin{center}
		a. \textbf{Nulidade de T} $= N(T) = dim_{\mathbb{K}}(Ker(T))$ \\
		b. \textbf{Posto de T} $= P(T) = dim_{\mathbb{K}}(Im(T))$
	\end{center}
	Seja $T: V_1 \rightarrow V_2$ uma transformação linear entre $\mathbb{K}$-espaços vetoriais.
	\begin{itemize}
		\item Se $T$ é uma função injetora, $T$ é um \textbf{monomorfismo}.
		\item Se $T$ é uma função sobrejetora, $T$ é um \textbf{epimorfismo}.
		\item Se $T$ é uma bijeção, $T$ é um \textbf{isomorfismo}.
		\item Se $V_1$ = $V_2$, $T$ é um \textbf{operador linear}, em vez de transformação linear.
	\end{itemize}

\newpage
\paragraph{Injetividade de Transformações Lineares\\}
	Seja $T: V_1 \rightarrow V_2$ uma transformação entre $\mathbb{K}$-espaços lineares.
	\begin{enumerate}
		\item $T$ é um monomorfismo se, e somente se $Ker(T) = \lbrace \overrightarrow{0} \rbrace$
		\item[•] Prova do item 1:
			\begin{center}
				$T(x) = T(y) \leftrightarrow T(x-y) = \overrightarrow{0} \leftrightarrow x - y \in Ker(T)$
			\end{center}
			Portanto, temos que $Ker(T) = \lbrace \overrightarrow{0} \rbrace$ se, e somente se $T(x) = T(y) \rightarrow x = y$
		\item Se $T$ é um isomorfismo linear, então $T^{-1}: V_2 \rightarrow V_1$ existe e é linear, e $dim_{\mathbb{K}}(V_1) = dim_{\mathbb{K}}(V_2)$
		\item[•] Prova do item 2:\\
			Sendo T uma bijeção, a existência da inversa é trivial, portanto, provaremos apenas sua linearidade.\\
			Seja $z = T(x)$, $w = T(y)$ e $\lambda \in \mathbb{K}$, logo, $T(x) + T(y) = z + w$, $T(\lambda x) = \lambda T(x)$, tem-se que:
			\begin{center}		
				$T^{-1}(z) = x$ e $T^{-1}(w) = y$ \\[5pt]
				$T^{-1}(z + w) = x + y = T^{-1}(z) + T^{-1}(w)$ \\[5pt]
				$T^{-1}(\lambda z) = \lambda T(z)$
			\end{center}
			O que prova a linearidade de $T$. Note que $T^{-1}$ é um isomorfismo linear.\\[5pt]
			Considere $n = dim_\mathbb{K}(V_1)$ e $\beta = \lbrace v_1, \dots, v_n \rbrace$ uma base de $V_1$. Como $T$ é sobrejetora, temos que $z \in V_2 \rightarrow \exists x \in V_1 (T(x) = z)$. Dado que $\beta$ é uma base de $V_1$, e considerando $\lambda_i \in \mathbb{K}$ podemos escrever $x$ como $x = \sum\limits^n_{i=1}\lambda_iv_i$, pela linearidade de $T$, temos $T(x) = \sum\limits^n_{i=1} T(\lambda_i v_i)
= \sum\limits^n_{i=1} \lambda_i T(v_i)$. Como não houve perda de generalidade na suposição de $z$, podemos escrever qualquer $z \in V_2$ como $T(\beta) = \lbrace T(v_1), \dots, T(v_n) \rbrace$, logo, temos que $T(\beta)$ é conjunto gerador de $V_2$, e, portanto, deste podemos extrair uma base, o que nos dá $dim_\mathbb{K}(V_1) = n = \vert \beta \vert = \vert T(\beta) \vert \geq dim_\mathbb{K}(V_2)$, no entanto, como $T^{-1}$ é um isomorfismo linear, sabemos que $dim_\mathbb{K}(V_2) \geq dim_\mathbb{K}(V_1)$, resultando em  $dim_\mathbb{K}(V_2) = dim_\mathbb{K}(V_1)$.
	\end{enumerate}
	
\paragraph{Transformações Lineares - Exemplos}
\begin{enumerate}
	\item Considere a função traço, $Tr: (M_n(\mathbb{K}),+) \rightarrow (\mathbb{K}, +)$, tal função recebe uma matriz e retorna a soma de todos os valores da sua diagonal principal $a_{ii}, i \in [1,n]$. Percebe-se que $Tr$ é um homomorfismo de grupos abelianos, e também uma transformação linear, pois: $Tr(\lambda A) = \lambda Tr(A)$, com $\lambda \in \mathbb{K}$, segue $Tr$ é um funcional linear (transformação do espaço vetorial ao corpo em qual o espaço está definido sobre).
	
	\item Seja $V = C(\mathbb{R},\mathbb{R})$ o espaço vetorial das funções contínuas de $\mathbb{R}$ em $\mathbb{R}$, sobre esse espaço, definimos a seguinte função:
	\begin{center}\framebox{
		$T(f) = \int\limits^1_0 f(x)dx$
	}\end{center}
	Dos cursos anteriores, sabemos que:
	\begin{center}
		$T(f + \lambda g) = \int\limits^1_0 \left[ f(x) + \lambda g(x)\right] dx = \int\limits^1_0 f(x)dx + \lambda \int\limits^1_0 g(x)dx$\\
		$= T(f) + \lambda T(g)$
	\end{center}
	Note que T é um \textbf{funcional} linear sobre $V$. $Ker(T)$ são todas as função cuja integral de 0 a 1 é igual a 0. Como $f(x) = \frac{1}{2} - x$.
	
	\item Seja $V = C^{\infty}(\mathbb{R},\mathbb{R})$ e $T: V \rightarrow V$ e definimos a função $T$ como
		\begin{center}
			$T(f) = f'$
		\end{center}
		Lembramos que:
		\begin{center}
			$T(f + \lambda g) = (f + \lambda g)' = f' + \lambda g' = T(f) + \lambda T(g)$
		\end{center}
		Verificado que $T$ é um homomorfismo de grupos abelianos (pois as operaçõe se mantém) sobre os quais a multiplicação por escalar vale, assim como para a função $T$, temos que, sabendo que o domínio e o contradomínio da função são os mesmos, $T$ é um operador linear.\\
		Note também que $Ker(T)$ é o conjunto das funções constantes, implicando $f' = 0$.
		
	\item Seja $T: \mathbb{R}^3 \rightarrow \mathbb{R}$, definida por $T(x,y,z) = x$ (a projeção na primeira coordenada).\\
	Sendo $u = (x, y, z)$ e $v = (a, b, c)$, ambos $\in \mathbb{R}^3$ e $\lambda \in \mathbb{R}$, temos
	\begin{center}
		$T(u + \lambda v) = T(x + \lambda a, y + \lambda b, z + \lambda c) = x + \lambda a = T(u) + \lambda T(v)$
	\end{center}
\end{enumerate}

\newpage
\section*{Aula 14 - Transformações Lineares (Parte 2)}
\paragraph{Teorema - Imagem/Núcleo em Posto/Nulidade\\}
	Seja $T: V_1 \rightarrow V_2$ uma transformação linear entre $\mathbb{K}$-espaços vetoriais de dimensão finita, então, temos
		\begin{enumerate}
		\item $dim(V_1) = dim(Ker(T)) + dim(Im(T))$
		\item $dim(V_1) = N(T) + P(T)$	
		\end{enumerate}
%%	Prova:\\
%%	Considere $T: V_1 \rightarrow V_2$ uma transformação entre $\mathbb{K}$-espaços vetoriais e $W = Ker(T)$. Como $W$ é subespaço de $V_1$, podemos construir o espaço vetorial quociente $\frac{V_1}{W}$, e sabemos que vale
%%	\begin{center}
%%		$dim(V_1) = dim(W) + dim(\frac{V_1}{W})$
%%	\end{center}
%%	Definimos uma segunda função $\widehat{T}$
%%	\begin{center}
%%		$\widehat{T}: \frac{V_1}{W} \rightarrow Im(T)$\\[5pt]
%%		$\widehat{T}([v]) = T(v)$
%%	\end{center}
%%	A função é bem definida, pois note que, se $[v] = [x]$, temos que $v \sim x$, portanto $v - x \in W = Ker(T)$.
%%	Logo, temos $\overrightarrow{0} = T(v - x) = T(v) - T(x)$ (pois $v - x \in Ker(T)$). Obtemos que $T(v) = T(x)$, logo, $%%\widehat{T}([v]) = T(v) = T(x) = \widehat{T}(x)$, logo, $\widehat{T}$ independe do representante da classe.\\[10pt]
%%	Por $[x] + [y] = [x + y]$ e $\lambda [x] = [\lambda x]$, temos que $\widehat{T}$ é uma transformação linear.		
	
\paragraph{Transformação Linear - Propriedade Fundamental\\}
	Seja $T: V \rightarrow W$ uma transformação linear entre $\mathbb{K}$-espaços vetoriais de dimensão $n$ e $m$, respectivamente. Seja $\alpha$ uma base $\lbrace v_1, \dots, v_n \rbrace$ de $V$.\\[5pt]
	Logo, dado $v \in V$ existem $x_1, \dots, x_n \in \mathbb{K}$ tais que
	\begin{center}
		$v = \sum\limits^n_{i = 1}x_i v_i$,\\[5pt] logo,\\[5pt]
		$T(v) = \sum\limits^n_{i=1} x_i T(v_i)$
	\end{center}
	Note que $(x_1, \dots, x_n) = [v]_\alpha$, logo, sabendo apenas $[v]_\alpha$ e $T(v_1), \dots, T(v_n)$, podemos saber a imagem de $v$ por $T$. Lembre-se que essa propriedade vale apenas para transformações lineares, pois depende de suas propriedades, como a linearidade.\\[10pt]
	\textbf{Exemplo}
	\begin{itemize}
	\item Seja $\beta = \lbrace (0,1), (1,0) \rbrace$ uma base de $\mathbb{R}^2$. Suponha que $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ é um funcional linear tal que $f(1,0) = 2$ e $f(0,1) = 1$. Então tem-se que:
		\begin{center}
			$f(x,y) = f(x'(1,0) + y'(0,1))$, porque $(x,y) = x'(1,0) + y'(0,1)$ e $(x',y') = [(x,y)]_\beta$ \\[10pt]
			$f(x'(1,0) + y'(0,1)) = f(x'(1,0)) + f(y'(0,1)) = x'f(1,0) + y'(0,1) = 2x' + y'$ 
		\end{center}	 
	\end{itemize}
	Estendemos agora essa propriedade, seja $T: V \rightarrow W$ uma transformação linear entre $\mathbb{K}$-espaços vetoriais de dimensão $n$ e $m$, respectivamente.\\
	Seja $\alpha = \lbrace v_1, \dots, v_n \rbrace$ uma base de $V$ e $\beta = \lbrace w_1, \dots, w_n \rbrace$ uma base de $W$.\\
	Então, dado $v \in V$, existem escalares $x_1, \dots, x_n \in \mathbb{K}$ tais que
	\begin{center}
		$v = \sum\limits_{i=1}^n x_i v_i$, em outras palavras,\\[5pt]
		$[v]_\alpha = (x_1, \dots, x_n)$
	\end{center}
	Temos, então:
	\begin{center}
		$T(v) = T \left( \sum\limits^n_{i=1} x_i v_i \right) = \sum\limits^n_{i=1}x_i T(v_i)$
	\end{center}
	E, para cada $i$, temos que $T(v_i) \in W$, e, portanto:
	\begin{center}
		$T(v_i) = \sum\limits^m_{j=1} a_{ij} w_j$, ou seja,\\[5pt]
		$[T(v_i)]_\beta  = (a_{i1}, \dots, a_{im})$
	\end{center}
	Podemos dizer, então:
	\begin{center}
		$T(v) = T \left( \sum\limits^n_{i=1} x_i v_i \right) = \sum\limits^n_{i=1}x_i T(v_i) = \sum\limits^n_{i=1}x_i \sum\limits^m_{j=1} a_{ij} w_j = \sum\limits^m_{j=1} \left( \sum\limits^n_{i=1}x_i a_{ij} \right) w_j$, logo,\\[5pt]
		$[T(v)]_\beta = \left( \sum\limits^n_{i=1} a_{i1} x_i, \dots, \sum\limits^n_{i=1} a_{im} x_i \right)$
	\end{center}
	De forma matricial, temos:
	\begin{center}
		$ [T(v)]_\beta =
		\begin{pmatrix}
		a_{11} & \dots & a_{1n} \\
		\vdots & \ddots & \vdots \\
		a_{m1} & \dots & a_{mn} 
		\end{pmatrix}
		[v]_\alpha$
	\end{center}
	Note que as colunas da matriz são representam, cada uma, $[T(v_i)]_\beta = (a_{i1}, \dots a_{i_m})$. Ela em si é uma matriz $m \times n \in M_{mn}(\mathbb{K})$ e a denotaremos como $[T]^\beta_\alpha$.\\
	Dessa forma, podemos escrever a relação
	\begin{center}
		$[T(v)]_\beta = [T]^\beta_\alpha \cdot [v]_\alpha$
	\end{center}
	Esta nos diz como obter as coordenadas de $T(v)$ sabendo apenas $[T]^\beta_\alpha$ e $[v]_\alpha$.\\[10pt]
	Podemos, então, interpretar $T$ como
	\begin{center}
		$T: \mathbb{K}^n \rightarrow \mathbb{K}^m$
	\end{center}
	e definir $T(X)$ como
	\begin{center}
		$T(X) = AX$ $\vert$ $A = [T]_\alpha^\beta$, $X = [v]_\alpha$
	\end{center}
	No caso em que $v \in Ker(T)$, temos
	\begin{center}
		$T(v) \in Ker(T) \Leftrightarrow T(v) = \overrightarrow{0} \Leftrightarrow [T(v)]_\beta = (0, \dots, 0)$	
	\end{center}
	Ou seja, para encontrar o núcleo de uma transformação linear entre espaços vetoriais de dimensão finita, basta resolvermos um sistema linear homogêneo.\\[10pt]
	Note que
	\begin{center}
		$Im(T) = Span_\mathbb{K} \left[ T(v_1), \dots, T(v_n) \right]$
	\end{center}
	Obtendo, assim, uma base da imagem $Im(T)$ dentro do conjunto $\left\lbrace T(v_1), \dots, T(v_n) \right\rbrace$.\\[10pt]
	\textbf{Exemplo\\}
	Suponha $\alpha = \lbrace x_1, \dots, x_n \rbrace$, $\beta = \lbrace a_1, \dots, a_n \rbrace$
	\begin{center}
		$v = x_1 v_1 + x_2 v_2$, e, logo, $[v]_\alpha = (x_1, x_2)$
	\end{center}
	A linearidade de $T$ implica em
	\begin{center}
		$T(v) = x_1 T(v_1) + x_2 T(v_2)$
	\end{center}
	Sabemos também que
	\begin{center}
		$T(v_1) = a_{11} w_1 + a_{21} w_2 + a_{31} w_3 $, logo $[T(v_1)]_\beta = (a_{11}, a_{21}, a_{31})$ \\
		$T(v_2) = a_{12} w_1 + a_{22} w_2 + a_{32} w_3 $, logo $[T(v_2)]_\beta = (a_{12}, a_{22}, a_{32})$
	\end{center}
	Obtemos
	\begin{center}
		$T(v) = T(x_1 v_1 + x_2 v_2) = x_1 T(v_1) + x_2 T(v_2)$\\
		$ = x_1 (a_{11} w_1 + a_{21} w_2 + a_{31} w_3) + x_2 (a_{12} w_1 + a_{22} w_2 + a_{32} w_3)$\\
		$ = w_1(x_1 a_{11} + x_2 a_{12}) + w_2(x_1 a_{21} + x_2 a_{22}) + w_3(x_1 a_{31} + x_2 a_{32}) $
	\end{center}
	Resultando em
	\begin{center}
		$[T(v)]_\beta = (x_1 a_{11} + x_2 a_{12}, x_1 a_{21} + x_2 a_{22}, x_1 a_{31} + x_2 a_{32}) = $\\[5pt]
		$\begin{pmatrix}
			a_{11} & a_{12} \\ a_{21} & a_{22} \\ a_{31} & a_{32}
		\end{pmatrix}
		\cdot
		\begin{pmatrix}
			x_1 \\ x_2
		\end{pmatrix}
		=
		\begin{pmatrix}
			a_{11} & a_{12} \\ a_{21} & a_{22} \\ a_{31} & a_{32}
		\end{pmatrix}
		\cdot
		[v]_\alpha
		$\\[5pt]
		Com $[T]_\alpha^\beta = \begin{pmatrix}
			a_{11} & a_{12} \\ a_{21} & a_{22} \\ a_{31} & a_{32}
		\end{pmatrix}$
	\end{center}
	Note que as colunas de $[T]^\beta_\alpha$ são as coordenadas de $T(v_1)$ e $T(v_2)$.\\
	Podemos interpretar $T$ como
	\begin{center}
		$T: \mathbb{R}^2 \rightarrow \mathbb{R}^3$\\[5pt]
		$T(x,y) = \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \\ a_{31} & a_{32} \end{pmatrix} \cdot \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}$
	\end{center}
	Temos, em particular
	\begin{center}
		$v \in Ker(T) \Leftrightarrow T(v) = \overrightarrow{0} \Leftrightarrow [T(v)]_\beta = (0, 0, 0)$\\[5pt]
		$[T]^\beta_\alpha \cdot \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}$
	\end{center}
	E, também
	\begin{center}
		$w \in Im(T) \Leftrightarrow \exists v \in \mathbb{R}^2( w = T(v) ) \Leftrightarrow \exists v \in V ([T(v)]_\beta = [w]_\beta)$\\ $\Leftrightarrow \exists v \in \mathbb{R}^2 ((y_1, y_2, y_3) = [w]_\beta = [T(v)]_\beta = [T]^\beta_\alpha [v]_\alpha$ \\[5pt]
		$[T]^\beta_\alpha \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = \begin{pmatrix} a_1 \\ a_2 \\ a_3 \end{pmatrix}$
	\end{center}
	Note que $ \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}$ é a solução do sistema linear.
	
\paragraph{Matriz associada a uma transformação linear\\}
	Sejam $V$ e $W$ $\mathbb{K}$-espaços vetoriais de dimensão $n$ e $m$, respectivamente, com $\alpha = \lbrace v_1, \dots, v_n \rbrace$ base de $V$ e $\beta$ uma base de W e $T: V \rightarrow W$ uma transformação linear.\\
	Logo, existe uma matriz $[T]^\beta_\alpha \in M_{mn}(\mathbb{K})$, tal que
	\begin{center}
		$\forall v \in V, [T(v)]_\beta = [T]^\beta_\alpha \cdot [v]_\alpha$
	\end{center}
	Sendo as colunas de $[T]^\beta_\alpha$ os escalares $[T(v_i)]_\beta$, com $1 \leq i \leq n$.\\[10pt]
	Tem-se que
	\begin{center}
		$v \in Ker(T) \Leftrightarrow [v]_\alpha$ é solução do sistema linear homogêneo $[T]^\beta_\alpha \cdot [v]_\alpha = \overrightarrow{0}$
	\end{center}
	Além disso
	\begin{center}
		$Im(T) = Span_\mathbb{K}\left[ T(v_1), \dots, T(v_n)\right]$
	\end{center}
	e \begin{center}
		$w \in Im(T) \subset W \Leftrightarrow \exists v \in V$\\
		$\Leftrightarrow [v]_\alpha$ é solução do sistema linear homogêneo $[T]^\beta_\alpha \cdot X = [w]_\beta$
	\end{center}
	\textbf{Exemplo\\}
	Sejam $T: P_2(\mathbb{R}) \rightarrow \mathbb{R}^2$ dada por
	\begin{center}
		$T(p(x)) = \left( \int\limits_{-1}^1 \right)$
	\end{center}
\end{document}




























